{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPGMLzBEnHgyXMcrQhpRJbE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tuevu110405/Machine-translation-Chinese-to-Vietnamese-/blob/main/Training_and_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ],
      "metadata": {
        "id": "w2NdK6nVFfQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dataset and needed packages"
      ],
      "metadata": {
        "id": "NPR_TdJ3pzWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1KuDLpUaSe0wzDWEhgG0wKXV1_AVmzLK4\n",
        "!unzip ./dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufu8n6JfY-JE",
        "outputId": "16f73e32-32b2-4f67-a04e-0df3ad05f5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KuDLpUaSe0wzDWEhgG0wKXV1_AVmzLK4\n",
            "To: /content/dataset.zip\n",
            "\r  0% 0.00/928k [00:00<?, ?B/s]\r100% 928k/928k [00:00<00:00, 141MB/s]\n",
            "Archive:  ./dataset.zip\n",
            "   creating: dataset/\n",
            "   creating: dataset/private_test/\n",
            "  inflating: dataset/private_test/private_test.zh  \n",
            "   creating: dataset/public_test/\n",
            "  inflating: dataset/public_test/public_test.zh  \n",
            "   creating: dataset/train/\n",
            "  inflating: dataset/train/train.vi  \n",
            "  inflating: dataset/train/train.zh  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "folder_id = '1DOyc8HUdz5z4XhuCG0xtR3wKc8bbS6_5'\n",
        "output_dir = os.getcwd()\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "!gdown --folder $folder_id -O $output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjgssEh-idU2",
        "outputId": "cadc699e-2cd1-4c38-fc04-bca61bbae7c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1ypF2QKRMdX3r82mJ4eAEP1gL_gpQbEe0 checkpoint_epoch_40.pt\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ypF2QKRMdX3r82mJ4eAEP1gL_gpQbEe0\n",
            "From (redirected): https://drive.google.com/uc?id=1ypF2QKRMdX3r82mJ4eAEP1gL_gpQbEe0&confirm=t&uuid=f02067a7-698f-4605-ac54-e419e8d810a6\n",
            "To: /content/checkpoint_epoch_40.pt\n",
            "100% 1.89G/1.89G [00:14<00:00, 130MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "output_dir = \"./checkpoint_bidirectional\"\n",
        "file_to_move = \"./checkpoint_epoch_40.pt\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "shutil.move(file_to_move, os.path.join(output_dir, os.path.basename(file_to_move)))\n",
        "print(f\"Moved {file_to_move} to {output_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWZkf2TYMCUD",
        "outputId": "7a9687ed-f10c-4192-de83-320e755d65be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moved ./checkpoint_epoch_40.pt to ./checkpoint_bidirectional\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPmYKgqL1Bx5",
        "outputId": "1f4d1385-52d6-4350-f523-da8f0e6d2c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Any, Dict, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import sacrebleu\n"
      ],
      "metadata": {
        "id": "OWBZQBqmce20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "b1gEZKgqqIe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \".\"\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, \"dataset\", \"train\")\n",
        "SRC_FILE = os.path.join(TRAIN_DIR, \"train.zh\")\n",
        "TGT_FILE = os.path.join(TRAIN_DIR, \"train.vi\")\n",
        "TOKENIZER_DIR = os.path.join(BASE_DIR, \"tokenizer_train32\")\n",
        "TOKENIZER_PREFIX = os.path.join(TOKENIZER_DIR, \"spm_zh_vi_joint\")\n",
        "MAX_TOKENS = 32\n",
        "VOCAB_SIZE = 8000\n",
        "USER_SYMBOLS = [\"<2zh>\", \"<2vi>\"]\n",
        "\n",
        "os.makedirs(TOKENIZER_DIR, exist_ok = True)\n",
        "\n",
        "def load_lines(path):\n",
        "    with open(path, \"r\", encoding = \"utf-8\") as f:\n",
        "        return [l.strip() for l in f if l.strip()]\n",
        "\n",
        "zh_lines = load_lines(SRC_FILE)\n",
        "vi_lines = load_lines(TGT_FILE)"
      ],
      "metadata": {
        "id": "5WPxXgM9XMBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Giả định các biến cấu hình đã được định nghĩa từ trước\n",
        "# TOKENIZER_DIR = \"./tokenizer\"\n",
        "# TOKENIZER_PREFIX = \"m_model\"\n",
        "# VOCAB_SIZE = 32000\n",
        "# USER_SYMBOLS = [\"<2vi>\", \"<2zh>\"] # Ví dụ các token đặc biệt\n",
        "\n",
        "# Train sentencepiece\n",
        "temp_corpus = os.path.join(TOKENIZER_DIR, \"temp_corpus.txt\")\n",
        "\n",
        "# Gộp dữ liệu tiếng Trung và Tiếng Việt vào chung 1 file để train Shared Vocabulary\n",
        "with open(temp_corpus, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for zh, vi in zip(zh_lines, vi_lines):\n",
        "        fout.write(zh + \"\\n\")\n",
        "        fout.write(vi + \"\\n\")\n",
        "\n",
        "# Configuration for SentencePiece\n",
        "# The spm_args string construction is no longer needed with keyword arguments.\n",
        "\n",
        "# Training\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=temp_corpus,\n",
        "    model_prefix=TOKENIZER_PREFIX,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    model_type=\"bpe\",\n",
        "    character_coverage=1.0,\n",
        "    pad_id=0,\n",
        "    unk_id=1,\n",
        "    bos_id=2,\n",
        "    eos_id=3,\n",
        "    user_defined_symbols=USER_SYMBOLS # Pass as a list directly\n",
        ")\n",
        "\n",
        "print(f\"Tokenizer saved to {TOKENIZER_PREFIX}.model\")\n",
        "\n",
        "#LOAD TOKENIZER\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(f\"{TOKENIZER_PREFIX}.model\")\n",
        "\n",
        "# 3. RELOAD RAW DATA (Tải lại dữ liệu gốc) -----\n",
        "def load_lines(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [l.strip() for l in f if l.strip()]\n",
        "\n",
        "full_zh_lines = load_lines(SRC_FILE)\n",
        "full_vi_lines = load_lines(TGT_FILE)\n",
        "assert len(zh_lines) == len(vi_lines)\n",
        "\n",
        "# --- 2. CHIA TẬP TRAIN / VAL / TEST (8:1:1) ---\n",
        "# Bước 1: Tách 80% cho Train, còn lại 20% cho Temp (Val + Test)\n",
        "train_zh, temp_zh, train_vi, temp_vi = train_test_split(\n",
        "    full_zh_lines, full_vi_lines, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Bước 2: Tách 20% Temp thành 10% Val và 10% Test (Chia đôi Temp)\n",
        "val_zh, test_zh, val_vi, test_vi = train_test_split(\n",
        "    temp_zh, temp_vi, test_size=0.5, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(train_zh)}\")\n",
        "print(f\"Val size:   {len(val_zh)}\")\n",
        "print(f\"Test size:  {len(test_zh)}\")\n",
        "\n",
        "#\n",
        "MAX_TOK = 64  # Ngưỡng độ dài tối đa (token)\n",
        "\n",
        "def tok_len(t, prefix=None):\n",
        "    \"\"\"Hàm tính độ dài token của một câu\"\"\"\n",
        "    if prefix:\n",
        "        t = f\"{prefix} {t}\" # Thêm token định hướng dịch (vd: <2vi>)\n",
        "    return len(sp.encode(t, out_type=int))\n",
        "\n",
        "# Tính độ dài trước khi lọc (chỉ để thống kê)\n",
        "zh_before = [tok_len(z, \"<2vi>\") for z in zh_lines]\n",
        "vi_before = [tok_len(v) for v in vi_lines]\n",
        "\n",
        "# Lọc bỏ các cặp câu quá dài\n",
        "filtered_zh, filtered_vi = [], []\n",
        "for z, v in zip(zh_lines, vi_lines):\n",
        "    # Chỉ giữ lại nếu CẢ nguồn và đích đều ngắn hơn MAX_TOK\n",
        "    if tok_len(z, \"<2vi>\") <= MAX_TOK and tok_len(v) <= MAX_TOK:\n",
        "        filtered_zh.append(z)\n",
        "        filtered_vi.append(v)\n",
        "\n",
        "# Chuyển sang numpy array để tiện xử lý thống kê sau này\n",
        "zh_after = np.array([tok_len(z, \"<2vi>\") for z in filtered_zh])\n",
        "vi_after = np.array([tok_len(v) for v in filtered_vi])\n",
        "\n",
        "OUT_DIR = \"./clean_data\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "zh_out = os.path.join(OUT_DIR, f\"train_maxlen{MAX_TOK}.zh\")\n",
        "vi_out = os.path.join(OUT_DIR, f\"train_maxlen{MAX_TOK}.vi\")\n",
        "\n",
        "with open(zh_out, \"w\", encoding=\"utf-8\") as fzh:\n",
        "    fzh.write(\"\\n\".join(filtered_zh))\n",
        "\n",
        "with open(vi_out, \"w\", encoding=\"utf-8\") as fvi:\n",
        "    fvi.write(\"\\n\".join(filtered_vi))"
      ],
      "metadata": {
        "id": "UgAgfcm6fnpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c485eb18-81ca-4fab-fc35-727fef80aeb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer saved to ./tokenizer_train32/spm_zh_vi_joint.model\n",
            "Train size: 25648\n",
            "Val size:   3206\n",
            "Test size:  3207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import sentencepiece as spm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class RopeConfig:\n",
        "    train_src_file: str\n",
        "    train_tgt_file: str\n",
        "    spm_prefix: str\n",
        "\n",
        "    vocab_size: int = 8000\n",
        "    d_model: int = 768\n",
        "    n_heads: int = 12\n",
        "    n_kv_heads: int = 4\n",
        "    num_encoder_layers: int = 8\n",
        "    num_decoder_layers: int = 8\n",
        "    d_ff: int = 3072\n",
        "    dropout: float = 0.01\n",
        "    max_len: int = 32\n",
        "    rope_base: float = 10000.0\n",
        "\n",
        "    pad_token: str = \"<pad>\"\n",
        "    bos_token: str = \"<s>\"\n",
        "    eos_token: str = \"</s>\"\n",
        "    unk_token: str = \"<unk>\"\n",
        "    zh_token: str = \"<2zh>\"\n",
        "    vi_token: str = \"<2vi>\"\n",
        "\n",
        "    #Training argument\n",
        "    batch_size  = 128\n",
        "    num_epochs = 40\n",
        "    lr_base = 2e-4\n",
        "    warmup_steps = 200\n",
        "    label_smoothing = 0.01\n",
        "    weight_decay: float = 0.01\n",
        "    grad_clip = 1.0\n",
        "    span_mask_prob = 0.01\n",
        "    vi2zh_epoch_ratio = 0.7\n",
        "    save_every: int = 3\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    num_workers = 8\n",
        "    save_dir = \"./checkpoint_bidirectional\"\n",
        "    seed = 42\n",
        "\n",
        "\n",
        "\n",
        "config = RopeConfig(\n",
        "    train_src_file=os.path.join(OUT_DIR, f\"train_maxlen{MAX_TOK}.zh\"),\n",
        "    train_tgt_file=os.path.join(OUT_DIR, f\"train_maxlen{MAX_TOK}.vi\"),\n",
        "    spm_prefix=TOKENIZER_PREFIX,\n",
        ")\n",
        "\n",
        "random.seed(config.seed)\n",
        "torch.manual_seed(config.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(config.seed)\n",
        "\n",
        "os.makedirs(config.save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "sJGkJVwYsRzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Dataset, DataLoader"
      ],
      "metadata": {
        "id": "jUlf9kRfrVm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from typing import List, Tuple, Dict, Any\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class BidirectionalTranslationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset sinh dữ liệu hai chiều (Zh->Vi và Vi->Zh)\n",
        "    kết hợp kỹ thuật Span Masking (tạo nhiễu).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_lines: List[str],\n",
        "        tgt_lines: List[str],\n",
        "        sp_model, # sentencepiece model\n",
        "        config,   # RopeConfig\n",
        "        is_training: bool = True,\n",
        "    ):\n",
        "        self.sp = sp_model\n",
        "        self.config = config\n",
        "        self.is_training = is_training\n",
        "\n",
        "        # Lấy ID của các token đặc biệt\n",
        "        self.pad_id = sp_model.piece_to_id(config.pad_token)\n",
        "        self.bos_id = sp_model.piece_to_id(config.bos_token)\n",
        "        self.eos_id = sp_model.piece_to_id(config.eos_token)\n",
        "        self.unk_id = sp_model.piece_to_id(config.unk_token)\n",
        "        self.zh_id = sp_model.piece_to_id(config.zh_token)\n",
        "        self.vi_id = sp_model.piece_to_id(config.vi_token)\n",
        "\n",
        "        self.samples = []\n",
        "\n",
        "        if is_training:\n",
        "            # Luân phiên tạo dữ liệu cho 2 hướng: Zh->Vi và Vi->Zh\n",
        "            for i, (src, tgt) in enumerate(zip(src_lines, tgt_lines)):\n",
        "                if i % 2 == 0:\n",
        "                    # Hướng xuôi: Input = Zh, Label = Vi\n",
        "                    # (CE_src_text, CE_tgt_text, CL_zh_text, CL_vi_text, direction)\n",
        "                    self.samples.append(\n",
        "                        (self.add_lang_token(src, config.vi_token), tgt, src, tgt, \"zh2vi\")\n",
        "                    )\n",
        "                else:\n",
        "                    # Hướng ngược: Input = Vi, Label = Zh\n",
        "                    # (CE_src_text, CE_tgt_text, CL_zh_text, CL_vi_text, direction)\n",
        "                    self.samples.append(\n",
        "                        (self.add_lang_token(tgt, config.zh_token), src, src, tgt, \"vi2zh\")\n",
        "                    )\n",
        "        else:\n",
        "            # Validation: Chỉ giữ hướng chính Zh->Vi\n",
        "            for src, tgt in zip(src_lines, tgt_lines):\n",
        "                self.samples.append(\n",
        "                    (self.add_lang_token(src, config.vi_token), tgt, src, tgt, \"zh2vi\")\n",
        "                )\n",
        "\n",
        "        # Lưu lại index của từng loại để dùng cho sampler sau này\n",
        "        self.zh2vi_indices = [\n",
        "            idx for idx, (_, _, _, _, d) in enumerate(self.samples) if d == \"zh2vi\"\n",
        "        ]\n",
        "        self.vi2zh_indices = [\n",
        "            idx for idx, (_, _, _, _, d) in enumerate(self.samples) if d == \"vi2zh\"\n",
        "        ]\n",
        "\n",
        "    def add_lang_token(self, text: str, lang_tok: str) -> str:\n",
        "        \"\"\"Thêm token ngôn ngữ vào đầu câu (vd: <2vi> Yêu em)\"\"\"\n",
        "        text = text.strip()\n",
        "        if text.startswith(\"<2vi>\") or text.startswith(\"<2zh>\"):\n",
        "            return text\n",
        "        return f\"{lang_tok} {text}\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, str]:\n",
        "        # Unpack all 5 items stored in self.samples\n",
        "        src_text_ce, tgt_text_ce, cl_zh_text, cl_vi_text, direction = self.samples[idx]\n",
        "\n",
        "        # Encode for Cross-Entropy Loss (main translation task)\n",
        "        src_ids_ce = self.sp.encode(src_text_ce, out_type=int)[: self.config.max_len]\n",
        "        tgt_ids_ce = [self.bos_id] + self.sp.encode(tgt_text_ce, out_type=int) + [self.eos_id]\n",
        "        tgt_ids_ce = tgt_ids_ce[: self.config.max_len]\n",
        "\n",
        "        # Apply span masking for CE source if training and direction is zh2vi\n",
        "        # Span masking should only apply to the actual source of the CE task\n",
        "        if self.is_training and direction == \"zh2vi\":\n",
        "            src_ids_ce = self.apply_span_masking(src_ids_ce)\n",
        "\n",
        "        # Encode for Contrastive Learning (original sentences without special tokens/masking)\n",
        "        # Ensure BOS/EOS/PAD for CL are handled consistently with model embedding layer expectations\n",
        "        cl_zh_ids = self.sp.encode(cl_zh_text, out_type=int)[:self.config.max_len] # No BOS/EOS here, as mean_pool expects clean sentences\n",
        "        cl_vi_ids = self.sp.encode(cl_vi_text, out_type=int)[:self.config.max_len]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(src_ids_ce, dtype=torch.long),\n",
        "            torch.tensor(tgt_ids_ce, dtype=torch.long),\n",
        "            torch.tensor(cl_zh_ids, dtype=torch.long),\n",
        "            torch.tensor(cl_vi_ids, dtype=torch.long),\n",
        "            direction\n",
        "        )\n",
        "\n",
        "    def apply_span_masking(self, src_ids: List[int]) -> List[int]:\n",
        "        \"\"\"\n",
        "        Thay thế ngẫu nhiên một đoạn token bằng <unk> với xác suất nhỏ.\n",
        "        Giúp mô hình học cách 'đoán' từ dựa trên ngữ cảnh (giống BERT).\n",
        "        \"\"\"\n",
        "        if not self.is_training or random.random() > self.config.span_mask_prob:\n",
        "            return src_ids\n",
        "\n",
        "        src_ids = src_ids.copy()\n",
        "        # Không mask các token đặc biệt\n",
        "        special = {self.pad_id, self.bos_id, self.eos_id, self.zh_id, self.vi_id}\n",
        "        maskable = [i for i, t in enumerate(src_ids) if t not in special]\n",
        "\n",
        "        if not maskable:\n",
        "            return src_ids\n",
        "\n",
        "        # Chọn độ dài đoạn mask (1 hoặc 2 token)\n",
        "        num_to_mask = min(random.randint(1, 2), len(maskable))\n",
        "        start = random.choice(maskable)\n",
        "\n",
        "        for i in range(start, min(start + num_to_mask, len(src_ids))):\n",
        "            # Mask với xác suất 70% trong vùng đã chọn\n",
        "            if i in maskable and random.random() < 0.7:\n",
        "                src_ids[i] = self.unk_id\n",
        "        return src_ids\n",
        "\n",
        "\n",
        "def select_vi2zh_window(indices: List[int], epoch: int, ratio: float) -> List[int]:\n",
        "    \"\"\"\n",
        "    Chọn một cửa sổ trượt (sliding window) các mẫu Vi->Zh.\n",
        "    Mỗi epoch sẽ chọn một phần khác nhau của dữ liệu dịch ngược.\n",
        "    \"\"\"\n",
        "    if not indices or ratio <= 0:\n",
        "        return []\n",
        "\n",
        "    total = len(indices)\n",
        "    # Tính kích thước cửa sổ dựa trên ratio (tỷ lệ)\n",
        "    window = max(1, int(math.ceil(total * min(ratio, 1.0))))\n",
        "\n",
        "    # Tính vị trí bắt đầu và kết thúc dựa trên số epoch (xoay vòng)\n",
        "    start = ((epoch - 1) * window) % total\n",
        "    end = start + window\n",
        "\n",
        "    if end <= total:\n",
        "        return indices[start:end]\n",
        "\n",
        "    # Xử lý trường hợp cửa sổ trượt qua cuối danh sách (nối phần cuối với phần đầu)\n",
        "    wrap = end - total\n",
        "    return indices[start:] + indices[:wrap]\n",
        "\n",
        "def collate_fn(batch): # This is the training collate function\n",
        "    \"\"\"\n",
        "    Batch đầu vào là list các tuple: (src_tensor_ce, tgt_tensor_ce, cl_zh_tensor, cl_vi_tensor, direction_string)\n",
        "    \"\"\"\n",
        "    # 1. Tách các thành phần từ batch (now expects 5 items from __getitem__)\n",
        "    src_ids_ce_list, tgt_ids_ce_list, cl_zh_ids_list, cl_vi_ids_list, directions = zip(*batch)\n",
        "\n",
        "    # 2. Tạo batch tổng cho Cross Entropy Loss (Padding bình thường)\n",
        "    PAD_ID = 0\n",
        "\n",
        "    src_batch_ce = pad_sequence(src_ids_ce_list, batch_first=True, padding_value=PAD_ID)\n",
        "    tgt_batch_ce = pad_sequence(tgt_ids_ce_list, batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "    # 3. Phân loại dữ liệu cho Contrastive Learning\n",
        "    zh_vi_src_cl = [] # Chinese source for Zh->Vi contrastive pairs\n",
        "    vi_vi_tgt_cl = [] # Vietnamese target for Zh->Vi contrastive pairs\n",
        "\n",
        "    vi_zh_src_cl = [] # Vietnamese source for Vi->Zh contrastive pairs\n",
        "    zh_zh_tgt_cl = [] # Chinese target for Vi->Zh contrastive pairs\n",
        "\n",
        "    for i in range(len(batch)):\n",
        "        direction = directions[i]\n",
        "        if direction == \"zh2vi\":\n",
        "            zh_vi_src_cl.append(cl_zh_ids_list[i])\n",
        "            vi_vi_tgt_cl.append(cl_vi_ids_list[i])\n",
        "        else: # vi2zh\n",
        "            vi_zh_src_cl.append(cl_vi_ids_list[i])\n",
        "            zh_zh_tgt_cl.append(cl_zh_ids_list[i])\n",
        "\n",
        "    # Helper để pad danh sách con (tránh lỗi nếu danh sách rỗng)\n",
        "    def safe_pad(tensor_list):\n",
        "        if not tensor_list:\n",
        "            # Trả về tensor rỗng kích thước [0, 1] để không lỗi model forward\n",
        "            return torch.zeros(0, 1, dtype=torch.long)\n",
        "        return pad_sequence(tensor_list, batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "    # 4. Đóng gói vào Dictionary\n",
        "    return {\n",
        "        'src': src_batch_ce,          # Dùng cho CE Loss (Tổng hợp)\n",
        "        'tgt': tgt_batch_ce,          # Dùng cho CE Loss (Tổng hợp)\n",
        "\n",
        "        # Dùng cho Contrastive Loss (Phân tách)\n",
        "        'ids_zh_vi': safe_pad(zh_vi_src_cl),\n",
        "        'ids_vi_vi': safe_pad(vi_vi_tgt_cl),\n",
        "\n",
        "        'ids_vi_zh': safe_pad(vi_zh_src_cl),\n",
        "        'ids_zh_zh': safe_pad(zh_zh_tgt_cl),\n",
        "    }\n",
        "\n",
        "def collate_fn_eval(batch): # This is the evaluation collate function\n",
        "    \"\"\"\n",
        "    Collate function for evaluation, expects 5 items from __getitem__ but only uses 2.\n",
        "    \"\"\"\n",
        "    # Unpack all 5 items returned by __getitem__\n",
        "    srcs_ce, tgts_ce, _, _, _ = zip(*batch) # Ignore CL-specific items and direction\n",
        "\n",
        "    PAD_ID = 0\n",
        "\n",
        "    max_src = max(len(s) for s in srcs_ce)\n",
        "    max_tgt = max(len(t) for t in tgts_ce)\n",
        "\n",
        "    src_pad = torch.zeros(len(batch), max_src, dtype = torch.long)\n",
        "    tgt_pad = torch.zeros(len(batch), max_tgt, dtype = torch.long)\n",
        "\n",
        "    for i, (s, t) in enumerate(zip(srcs_ce, tgts_ce)):\n",
        "        src_pad[i, :len(s)] = s\n",
        "        tgt_pad[i, :len(t)] = t\n",
        "\n",
        "    return src_pad, tgt_pad\n",
        "\n",
        "def build_train_loader(\n",
        "    train_dataset: BidirectionalTranslationDataset,\n",
        "    epoch: int,\n",
        "    config,\n",
        ") -> Tuple[DataLoader, int]:\n",
        "    \"\"\"\n",
        "    Tạo DataLoader kết hợp:\n",
        "    1. Toàn bộ dữ liệu chính (Zh->Vi)\n",
        "    2. Một phần dữ liệu phụ (Vi->Zh) thay đổi theo epoch\n",
        "    \"\"\"\n",
        "    # Lấy toàn bộ index của hướng xuôi\n",
        "    active = list(train_dataset.zh2vi_indices)\n",
        "\n",
        "    # Lấy một phần index của hướng ngược\n",
        "    vi_slice = select_vi2zh_window(\n",
        "        train_dataset.vi2zh_indices, epoch, config.vi2zh_epoch_ratio\n",
        "    )\n",
        "    active.extend(vi_slice)\n",
        "\n",
        "    # Dùng SubsetRandomSampler để chỉ lấy mẫu trong danh sách 'active'\n",
        "    sampler = SubsetRandomSampler(active)\n",
        "\n",
        "    loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        sampler=sampler,\n",
        "        collate_fn=collate_fn, # Use the updated training collate_fn\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    return loader, len(vi_slice)\n"
      ],
      "metadata": {
        "id": "2UScQZtex2Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BidirectionalTranslationDataset(\n",
        "    train_zh, train_vi, sp, config, is_training=True\n",
        ")\n",
        "\n",
        "# Tập Val & Test: is_training=False (Chỉ giữ chiều xuôi Zh->Vi, không nhiễu)\n",
        "val_dataset = BidirectionalTranslationDataset(\n",
        "    val_zh, val_vi, sp, config, is_training=False\n",
        ")\n",
        "\n",
        "test_dataset = BidirectionalTranslationDataset(\n",
        "    test_zh, test_vi, sp, config, is_training=False\n",
        ")\n",
        "\n",
        "# B. Khởi tạo DataLoader\n",
        "# Train Loader: Cần dùng hàm đặc biệt để xử lý sliding window\n",
        "# (Sẽ gọi lại trong vòng lặp huấn luyện từng epoch)\n",
        "# train_loader, _ = build_train_loader(train_dataset, epoch=1, config=config)\n",
        "\n",
        "# Val & Test Loader: Dùng DataLoader tiêu chuẩn (không cần sampler phức tạp)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "IwuHf7pfSS9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "2eKbc9vVyWDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-8):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        rms = torch.sqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)\n",
        "        return self.weight * x / rms\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, d_model: int, base: float = 10000.0):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, d_model, 2).float() / d_model))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "        self._cos = None\n",
        "        self._sin = None\n",
        "        self._seq_len_cached = 0\n",
        "\n",
        "    def _maybe_update_cache(self, seq_len, device, dtype):\n",
        "        if seq_len > self._seq_len_cached or self._cos is None or self._cos.device != device:\n",
        "            self._seq_len_cached = seq_len\n",
        "            positions = torch.arange(seq_len, device = device, dtype = dtype)\n",
        "            freqs = torch.outer(positions, self.inv_freq.to(device))\n",
        "            self._cos = torch.cos(freqs)\n",
        "            self._sin = torch.sin(freqs)\n",
        "\n",
        "    def forward(self, x, seq_len):\n",
        "        seq_len = seq_len or x.size(-2)\n",
        "        self._maybe_update_cache(seq_len, x.device, x.dtype)\n",
        "        # Corrected: RoPE should return cos and sin components, not multiply x\n",
        "        return self._cos[:seq_len], self._sin[:seq_len]\n",
        "\n",
        "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    # x shape: (..., seq_len, head_dim)\n",
        "    head_dim = x.shape[-1]\n",
        "    x1 = x[..., 0::2]\n",
        "    x2 = x[..., 1::2]\n",
        "\n",
        "    # Reshape cos, sin để broadcast đúng: (1, 1, seq_len, head_dim/2)\n",
        "    # Lưu ý: cos, sin từ hàm RoPE của bạn đang có shape (seq_len, head_dim/2)\n",
        "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
        "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    rot1 = x1 * cos - x2 * sin\n",
        "    rot2 = x1 * sin + x2 * cos\n",
        "    return torch.cat([rot1, rot2], dim=-1)\n",
        "\n",
        "\n",
        "class FFN_SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.d_ff = d_ff  # <--- Thêm dòng này\n",
        "        self.linear1 = nn.Linear(d_model, 2 * d_ff, bias=False)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.linear1(x)\n",
        "        g, v = h[..., :self.d_ff], h[..., self.d_ff:]\n",
        "        # Công thức đúng: (SiLU(g) * v)\n",
        "        # F.silu(g) tương đương g * sigmoid(g)\n",
        "        s = F.silu(g) * v  # <--- Sửa logic nhân với v\n",
        "        out = self.linear2(s)\n",
        "        return self.dropout(out)\n",
        "\n",
        "class GroupedQueryAttentionRoPE(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, n_kv_heads, dropout, rope_base):\n",
        "        super().__init__()\n",
        "        assert n_heads % n_kv_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        self.n_kv_heads = n_kv_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "        # Corrected: Define n_groups\n",
        "        self.n_groups = self.n_heads // self.n_kv_heads\n",
        "        self.W_q = nn.Linear(d_model, n_heads * self.d_k, bias = False)\n",
        "        self.W_k = nn.Linear(d_model, n_kv_heads * self.d_k, bias = False)\n",
        "        self.W_v = nn.Linear(d_model, n_kv_heads * self.d_k, bias = False)\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = math.sqrt(self.d_k)\n",
        "        self.rope = RoPE(self.d_k, base = rope_base)\n",
        "\n",
        "    def forward(self, q, k, v, key_padding_mask = None, attn_mask = None):\n",
        "        B, T_q = q.size(0), q.size(1)\n",
        "        T_k = k.size(1)\n",
        "\n",
        "        Q = self.W_q(q).view(B, T_q, self.n_heads, self.d_k).transpose(1,2)\n",
        "        K = self.W_k(k).view(B, T_k, self.n_kv_heads, self.d_k).transpose(1,2)\n",
        "        V = self.W_v(v).view(B, T_k, self.n_kv_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "        cos_q, sin_q = self.rope(Q, T_q)\n",
        "        cos_k, sin_k = self.rope(K, T_k)\n",
        "\n",
        "        Q = apply_rope(Q, cos_q, sin_q)\n",
        "        K = apply_rope(K, cos_k, sin_k)\n",
        "\n",
        "        K = K.repeat_interleave(self.n_groups, dim = 1)\n",
        "        V = V.repeat_interleave(self.n_groups, dim = 1)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "        if key_padding_mask is not None:\n",
        "            scores = scores.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float(\"-inf\"))\n",
        "        if attn_mask is not None:\n",
        "            # Corrected: Remove unsqueeze calls to allow proper broadcasting of (T, T) mask\n",
        "            scores = scores.masked_fill(attn_mask, float(\"-inf\"))\n",
        "        attn = F.softmax(scores, dim = -1)\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.matmul(attn, V)\n",
        "        out = out.transpose(1,2).contiguous().view(B, T_q, -1)\n",
        "        return self.W_o(out)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config: RopeConfig):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.self_attn = GroupedQueryAttentionRoPE(\n",
        "            config.d_model,\n",
        "            config.n_heads,\n",
        "            config.n_kv_heads,\n",
        "            config.dropout,\n",
        "            config.rope_base\n",
        "        )\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.ffn = FFN_SwiGLU(config.d_model, config.d_ff, config.dropout)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, src_pad_mask = None):\n",
        "        x1 = self.ln1(x)\n",
        "        attn = self.self_attn(x1, x1, x1, key_padding_mask = src_pad_mask)\n",
        "        x = x + self.dropout(attn)\n",
        "        x2 = self.ln2(x)\n",
        "        return x + self.ffn(x2)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: RopeConfig):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.self_attn = GroupedQueryAttentionRoPE(\n",
        "            config.d_model,\n",
        "            config.n_heads,\n",
        "            config.n_kv_heads,\n",
        "            config.dropout,\n",
        "            config.rope_base\n",
        "        )\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.cross_attn = GroupedQueryAttentionRoPE(\n",
        "            config.d_model, config.n_heads, config.n_kv_heads, config.dropout, config.rope_base\n",
        "        )\n",
        "        self.ln3 = RMSNorm(config.d_model)\n",
        "        self.ffn = FFN_SwiGLU(config.d_model, config.d_ff, config.dropout)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, y, enc_out, tgt_pad_mask = None, tgt_casual_mask = None, src_pad_mask = None):\n",
        "        y1 = self.ln1(y)\n",
        "        # Use self attention with casual mask\n",
        "        self_attn = self.self_attn(y1, y1, y1,\n",
        "                                   key_padding_mask = tgt_pad_mask,\n",
        "        attn_mask = tgt_casual_mask)\n",
        "        y = y + self.dropout(self_attn)\n",
        "        y2 = self.ln2(y)\n",
        "        #Cross attention (K, V is the output of the encoder)\n",
        "        cross_attn = self.cross_attn(y2, enc_out, enc_out,\n",
        "                                      key_padding_mask = src_pad_mask)\n",
        "        y = y + self.dropout(cross_attn)\n",
        "        y3 = self.ln3(y)\n",
        "        return y + self.ffn(y3)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, config: RopeConfig, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embedding = nn.Embedding(vocab_size, config.d_model, padding_idx = 0\n",
        "                                    )\n",
        "        self.emb_dropout = nn.Dropout(config.dropout)\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.num_encoder_layers)])\n",
        "        self.encoder_final_ln = RMSNorm(config.d_model)\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.num_decoder_layers)])\n",
        "        self.decoder_final_ln = RMSNorm(config.d_model)\n",
        "\n",
        "        self.output_bias = nn.Parameter(torch.zeros(vocab_size))\n",
        "        self.emb_scale = math.sqrt(config.d_model)\n",
        "        self._init_weight()\n",
        "\n",
        "    def _init_weight(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "    def forward(self, src_ids: torch.Tensor, tgt_ids: torch.Tensor):\n",
        "        src_pad = (src_ids == 0)\n",
        "        tgt_pad = (tgt_ids == 0)\n",
        "\n",
        "        tgt_in = tgt_ids[:, : -1]\n",
        "        tgt_pad_in = tgt_pad[:, : -1]\n",
        "        T = tgt_in.size(1)\n",
        "        tgt_causal = torch.triu(\n",
        "            torch.ones(T, T, dtype = torch.bool, device = tgt_in.device), diagonal = 1\n",
        "        )\n",
        "\n",
        "        src_emb = self.emb_dropout(self.embedding(src_ids) * self.emb_scale)\n",
        "        enc_out = src_emb\n",
        "        for layer in self.encoder_layers:\n",
        "            enc_out = layer(enc_out,src_pad)\n",
        "        enc_out = self.encoder_final_ln(enc_out)\n",
        "\n",
        "        tgt_emb = self.emb_dropout(self.embedding(tgt_in) * self.emb_scale)\n",
        "        dec_out = tgt_emb\n",
        "        for layer in self.decoder_layers:\n",
        "            dec_out = layer(dec_out, enc_out, tgt_pad_in, tgt_causal, src_pad)\n",
        "        dec_out = self.decoder_final_ln(dec_out)\n",
        "\n",
        "        return F.linear(dec_out, self.embedding.weight, self.output_bias)"
      ],
      "metadata": {
        "id": "X8CSsRgbz08B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criterion"
      ],
      "metadata": {
        "id": "aD7cd1pHsZ0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self,smoothing: float = 0.1, ignore_index: int = 0):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        vocab_size = logits.size(-1)\n",
        "        log_probs = F.log_softmax(logits, dim = -1)\n",
        "\n",
        "        mask = targets != self.ignore_index\n",
        "\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.full_like(\n",
        "                log_probs, self.smoothing / (vocab_size - 1)\n",
        "            )\n",
        "            true_dist.scatter_(1, targets.data.unsqueeze(1), 1 - self.smoothing)\n",
        "            true_dist[targets == self.ignore_index] = 0.0\n",
        "\n",
        "        loss = -(true_dist * log_probs).sum(dim=-1)\n",
        "        loss = loss.masked_fill(~mask,0.0)\n",
        "        return loss.sum() / mask.sum().clamp(min=1)"
      ],
      "metadata": {
        "id": "-C8wJ7FCUQKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "ydULG5GatL3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmupInverseSqrtScheduler:\n",
        "    def __init__(self, optimizer, warmup_steps: int, lr_base: float):\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.lr_base = lr_base\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        if self.step_num <= self.warmup_steps:\n",
        "            lr = self.lr_base * self.step_num / self.warmup_steps\n",
        "        else:\n",
        "            lr = self.lr_base * math.sqrt(self.warmup_steps) / math.sqrt(self.step_num)\n",
        "        for group in self.optimizer.param_groups:\n",
        "            group[\"lr\"] = lr\n",
        "\n",
        "    def get_lr(self):\n",
        "        return self.optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, scheduler, config: RopeConfig, epoch: int) -> float:\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
        "    for src_batch, tgt_batch in pbar:\n",
        "        src_batch = src_batch.to(config.device)\n",
        "        tgt_batch = tgt_batch.to(config.device)\n",
        "        logits = model(src_batch, tgt_batch)\n",
        "        targets = tgt_batch[:, 1:]\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{scheduler.get_lr():.6f}\")\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BeamSearchHypothesis:\n",
        "    tokens: list\n",
        "    log_prob: float\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode(\n",
        "    model: TransformerModel,\n",
        "    src_ids: torch.Tensor,\n",
        "    sp_model: spm.SentencePieceProcessor,\n",
        "    config: RopeConfig,\n",
        "    max_len: int = 32,\n",
        "):\n",
        "    \"\"\"Fast greedy decoding for validation BLEU calculation\"\"\"\n",
        "    model.eval()\n",
        "    batch_size = src_ids.size(0)\n",
        "    device = src_ids.device\n",
        "    bos = sp_model.piece_to_id(config.bos_token)\n",
        "    eos = sp_model.piece_to_id(config.eos_token)\n",
        "    pad = sp_model.piece_to_id(config.pad_token)\n",
        "\n",
        "    # Encode source\n",
        "    src_pad_mask = (src_ids == pad)\n",
        "    src_emb = model.embedding(src_ids) * model.emb_scale\n",
        "    src_input = model.emb_dropout(src_emb)\n",
        "    enc_out = src_input\n",
        "    for layer in model.encoder_layers:\n",
        "        enc_out = layer(enc_out, src_pad_mask)\n",
        "    enc_out = model.encoder_final_ln(enc_out)\n",
        "\n",
        "    # Initialize decoder input\n",
        "    tgt_ids = torch.full((batch_size, 1), bos, dtype=torch.long, device=device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tgt_pad = (tgt_ids == pad)\n",
        "        tgt_len = tgt_ids.size(1)\n",
        "        tgt_causal = torch.triu(torch.ones(tgt_len, tgt_len, dtype=torch.bool, device=device), diagonal=1)\n",
        "\n",
        "        tgt_emb = model.embedding(tgt_ids) * model.emb_scale\n",
        "        tgt_input = model.emb_dropout(tgt_emb)\n",
        "        dec_out = tgt_input\n",
        "        for layer in model.decoder_layers:\n",
        "            dec_out = layer(dec_out, enc_out, tgt_pad, tgt_causal, src_pad_mask)\n",
        "        dec_out = model.decoder_final_ln(dec_out)\n",
        "\n",
        "        logits = F.linear(dec_out, model.embedding.weight, model.output_bias)\n",
        "        next_tokens = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        tgt_ids = torch.cat([tgt_ids, next_tokens], dim=1)\n",
        "\n",
        "        # Stop if all sequences have EOS\n",
        "        if (next_tokens.squeeze(-1) == eos).all():\n",
        "            break\n",
        "\n",
        "    # Decode to text\n",
        "    decoded = []\n",
        "    for seq in tgt_ids:\n",
        "        tokens = seq[1:].cpu().tolist()  # Skip BOS\n",
        "        if eos in tokens:\n",
        "            tokens = tokens[:tokens.index(eos)]\n",
        "        decoded.append(sp_model.decode(tokens))\n",
        "\n",
        "    return decoded\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def beam_search_decode(\n",
        "    model: TransformerModel,\n",
        "    src_ids: torch.Tensor,\n",
        "    sp_model: spm.SentencePieceProcessor,\n",
        "    config: RopeConfig,\n",
        "    beam_size: int = 3,\n",
        "    top_k: int = 3,\n",
        "    max_len: int = 32,\n",
        "    length_penalty: float = 0.6,\n",
        "):\n",
        "    model.eval()\n",
        "    batch_size = src_ids.size(0)\n",
        "    device = src_ids.device\n",
        "    bos = sp_model.piece_to_id(config.bos_token)\n",
        "    eos = sp_model.piece_to_id(config.eos_token)\n",
        "    pad = sp_model.piece_to_id(config.pad_token)\n",
        "    src_pad_mask = (src_ids == pad)\n",
        "    src_emb = model.embedding(src_ids) * model.emb_scale\n",
        "    src_input = model.emb_dropout(src_emb)\n",
        "    enc_out = src_input\n",
        "    for layer in model.encoder_layers:\n",
        "        enc_out = layer(enc_out, src_pad_mask)\n",
        "    enc_out = model.encoder_final_ln(enc_out)\n",
        "    decoded = []\n",
        "    for b in range(batch_size):\n",
        "        beams = [BeamSearchHypothesis(tokens=[bos], log_prob=0.0)]\n",
        "        finished = []\n",
        "        single_enc = enc_out[b : b + 1]\n",
        "        single_mask = src_pad_mask[b : b + 1]\n",
        "        for _ in range(max_len):\n",
        "            new_beams = []\n",
        "            for beam in beams:\n",
        "                if beam.tokens[-1] == eos:\n",
        "                    finished.append(beam)\n",
        "                    continue\n",
        "                tgt_ids = torch.tensor(beam.tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "                tgt_pad = (tgt_ids == pad)\n",
        "                tgt_len = tgt_ids.size(1)\n",
        "                tgt_causal = torch.triu(torch.ones(tgt_len, tgt_len, dtype=torch.bool, device=device), diagonal=1)\n",
        "                tgt_emb = model.embedding(tgt_ids) * model.emb_scale\n",
        "                tgt_input = model.emb_dropout(tgt_emb)\n",
        "                dec_out = tgt_input\n",
        "                for layer in model.decoder_layers:\n",
        "                    dec_out = layer(dec_out, single_enc, tgt_pad, tgt_causal, single_mask)\n",
        "                dec_out = model.decoder_final_ln(dec_out)\n",
        "                logits = F.linear(dec_out, model.embedding.weight, model.output_bias)\n",
        "                log_probs = F.log_softmax(logits[:, -1, :], dim=-1)\n",
        "                if top_k and top_k > 0:\n",
        "                    top_val, top_idx = torch.topk(log_probs, min(top_k, log_probs.size(-1)))\n",
        "                    mask = torch.full_like(log_probs, float(\"-inf\"))\n",
        "                    mask.scatter_(1, top_idx, top_val)\n",
        "                    log_probs = mask\n",
        "                top_vals, top_idx = torch.topk(log_probs.squeeze(0), beam_size)\n",
        "                for val, idx in zip(top_vals.tolist(), top_idx.tolist()):\n",
        "                    new_beams.append(BeamSearchHypothesis(tokens=beam.tokens + [idx], log_prob=beam.log_prob + val))\n",
        "            beams = sorted(new_beams, key=lambda h: h.log_prob, reverse=True)[:beam_size]\n",
        "            if not beams:\n",
        "                break\n",
        "        finished.extend(beams)\n",
        "        best = max(finished, key=lambda h: h.log_prob / (len(h.tokens) ** length_penalty))\n",
        "        tokens = best.tokens[1:]\n",
        "        if eos in tokens:\n",
        "            tokens = tokens[: tokens.index(eos)]\n",
        "        decoded.append(sp_model.decode(tokens))\n",
        "    return decoded\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, criterion, sp_model, config: RopeConfig, calculate_bleu: bool = True, max_bleu_samples: int = 500):\n",
        "    \"\"\"\n",
        "    Fast evaluation with optional BLEU calculation\n",
        "    Args:\n",
        "        calculate_bleu: Whether to calculate BLEU score (slower)\n",
        "        max_bleu_samples: Maximum number of samples for BLEU calculation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_predictions, all_references = [], []\n",
        "\n",
        "    bleu_count = 0\n",
        "    for i, (src_batch, tgt_batch) in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
        "        src_batch = src_batch.to(config.device)\n",
        "        tgt_batch = tgt_batch.to(config.device)\n",
        "\n",
        "        # Always calculate loss\n",
        "        logits = model(src_batch, tgt_batch)\n",
        "        targets = tgt_batch[:, 1:]\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Only calculate BLEU on subset for speed\n",
        "        if calculate_bleu and bleu_count < max_bleu_samples:\n",
        "            # Use greedy decoding instead of beam search for speed\n",
        "            preds = greedy_decode(model, src_batch, sp_model, config, max_len=config.max_len)\n",
        "\n",
        "            eos = sp_model.piece_to_id(config.eos_token)\n",
        "            for tgt_seq in tgt_batch:\n",
        "                ref = tgt_seq[1:].cpu().tolist()\n",
        "                if eos in ref:\n",
        "                    ref = ref[: ref.index(eos)]\n",
        "                all_references.append(sp_model.decode(ref))\n",
        "\n",
        "            all_predictions.extend(preds)\n",
        "            bleu_count += len(preds)\n",
        "\n",
        "            # Stop if we have enough samples for BLEU\n",
        "            if bleu_count >= max_bleu_samples:\n",
        "                break\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    bleu_score = 0.0\n",
        "\n",
        "    if calculate_bleu and sacrebleu is not None and all_predictions:\n",
        "        try:\n",
        "            bleu = sacrebleu.corpus_bleu(all_predictions, [all_references], force=True)\n",
        "            bleu_score = bleu.score\n",
        "        except Exception as err:\n",
        "            print(f\"BLEU calculation failed: {err}\")\n",
        "\n",
        "    return avg_loss, bleu_score"
      ],
      "metadata": {
        "id": "r3SJc7kR7DhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, input_dim, proj_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim, proj_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(self.net(x), dim = -1)\n",
        "\n",
        "def encode_context(model, src_ids, pad_id):\n",
        "    src_pad_mask = (src_ids == pad_id)\n",
        "    src_emb = model.embedding(src_ids) * model.emb_scale\n",
        "    src_input = model.emb_dropout(src_emb)\n",
        "\n",
        "    enc_out = src_input\n",
        "    for layer in model.encoder_layers:\n",
        "        enc_out = layer(enc_out, src_pad_mask)\n",
        "    enc_out = model.encoder_final_ln(enc_out)\n",
        "    return enc_out\n",
        "\n",
        "def mean_pool(enc_out, ids, pad_id, special_ids):\n",
        "    mask = (ids != pad_id)\n",
        "    for special in special_ids:\n",
        "        mask = mask & (ids != special)\n",
        "    mask = mask.float()\n",
        "\n",
        "    summed = (enc_out * mask.unsqueeze(-1)).sum(dim = 1)\n",
        "    denom = mask.sum(dim = 1, keepdim=True).clamp(min = 1) # Corrected 'keepsim' to 'keepdim'\n",
        "    return summed/denom\n",
        "\n",
        "def contrastive_loss(z_a, z_b, tau):\n",
        "    sim = torch.matmul(z_a, z_b.transpose(0,1))/tau\n",
        "    labels = torch.arange(sim.size(0), device = sim.device)\n",
        "    loss_i = F.cross_entropy(sim, labels)\n",
        "    loss_j = F.cross_entropy(sim.transpose(0,1), labels)\n",
        "    return (loss_i + loss_j)/2\n",
        "\n",
        "def compute_crosslingual_loss(model,\n",
        "                              projection,\n",
        "                              ids_zh_vi,\n",
        "                              ids_vi_vi,\n",
        "                              ids_vi_zh,\n",
        "                              ids_zh_zh,\n",
        "                              cl_config,\n",
        "                              pad_id,\n",
        "                              special_ids):\n",
        "    if ids_zh_vi.size(0) < 2:\n",
        "        return torch.zeros(1, device = cl_config.device)\n",
        "\n",
        "    enc_zh_vi = encode_context(model, ids_zh_vi, pad_id)\n",
        "    enc_vi_vi = encode_context(model, ids_vi_vi, pad_id)\n",
        "    enc_vi_zh = encode_context(model, ids_vi_zh, pad_id)\n",
        "    enc_zh_zh = encode_context(model, ids_zh_zh, pad_id)\n",
        "\n",
        "    z_zh_vi = projection(mean_pool(enc_zh_vi, ids_zh_vi, pad_id, special_ids))\n",
        "    z_vi_vi = projection(mean_pool(enc_vi_vi, ids_vi_vi, pad_id, special_ids))\n",
        "    z_vi_zh = projection(mean_pool(enc_vi_zh, ids_vi_zh, pad_id, special_ids))\n",
        "    z_zh_zh = projection(mean_pool(enc_zh_zh, ids_zh_zh, pad_id, special_ids))\n",
        "\n",
        "    cl_vi_space = contrastive_loss(z_zh_vi, z_vi_vi, cl_config.contrastive_tau)\n",
        "    cl_zh_space = contrastive_loss(z_vi_zh, z_zh_zh, cl_config.contrastive_tau)\n",
        "\n",
        "    return (cl_vi_space + cl_zh_space) * 0.5\n",
        "\n",
        "def contrastive_train_epoch(model, projection, dataloader,optimizer, criterion, scheduler, cl_config, pad_id, special_ids, global_step, epoch):\n",
        "    model.train()\n",
        "    projection.train()\n",
        "\n",
        "    total_ce_loss = 0.0\n",
        "    total_cl_loss = 0.0\n",
        "    total_loss = 0.0\n",
        "    pbar = tqdm(dataloader, desc = f\"Epoch {epoch}\")\n",
        "    for batch in pbar:\n",
        "        src = batch['src'].to(cl_config.device)\n",
        "        tgt = batch['tgt'].to(cl_config.device)\n",
        "        ids_zh_vi = batch['ids_zh_vi'].to(cl_config.device)\n",
        "        ids_vi_zh = batch['ids_vi_zh'].to(cl_config.device)\n",
        "        ids_zh_zh = batch['ids_zh_zh'].to(cl_config.device)\n",
        "        ids_vi_vi = batch['ids_vi_vi'].to(cl_config.device)\n",
        "\n",
        "\n",
        "        tgt_label = tgt[:, 1:]  # Bỏ token đầu (để dự đoán)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(src, tgt)\n",
        "\n",
        "        # Tuy nhiên, Label để tính loss thì ta phải tự cắt bên ngoài\n",
        "        tgt_label = tgt[:, 1:] # Bỏ token đầu <bos> để làm nhãn thực tế\n",
        "\n",
        "        ce_loss = criterion(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            tgt_label.reshape(-1)\n",
        "        )\n",
        "\n",
        "        warmup_ratio = min(1.0, global_step / max(1, cl_config.cross_warmup_steps))\n",
        "        lambda_cross = cl_config.cross_lambda_max * warmup_ratio\n",
        "\n",
        "        if ids_zh_vi.size(0) > 0 and ids_vi_zh.size(0) > 0:\n",
        "             cl_loss = compute_crosslingual_loss(\n",
        "                model, projection,\n",
        "                ids_zh_vi, ids_vi_vi, ids_vi_zh, ids_zh_zh, # Lưu ý thứ tự tham số\n",
        "                cl_config, pad_id, special_ids\n",
        "            )\n",
        "        else:\n",
        "            # Nếu batch thiếu dữ liệu để đối chiếu, bỏ qua CL loss batch này\n",
        "            cl_loss = torch.tensor(0.0, device=cl_config.device)\n",
        "\n",
        "        total_batch_loss = ce_loss + lambda_cross * cl_loss\n",
        "        total_batch_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(list(model.parameters()) + list(projection.parameters()), config.grad_clip)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_ce_loss += ce_loss.item()\n",
        "        total_cl_loss += cl_loss.item()\n",
        "        total_loss += total_batch_loss.item()\n",
        "        global_step += 1\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'ce': f'{ce_loss.item():.4f}',\n",
        "            'cl': f'{cl_loss.item():.4f}',\n",
        "            'λ': f'{lambda_cross:.3f}',\n",
        "            'lr': f'{scheduler.get_lr():.6f}'\n",
        "        })\n",
        "\n",
        "    return total_ce_loss / len(dataloader), total_cl_loss / len(dataloader), total_loss / len(dataloader),global_step"
      ],
      "metadata": {
        "id": "RB2_ovOdW6X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training stage 1\n",
        "- Objective function: Cross Entropy loss\n",
        "- Training for 40 epochs"
      ],
      "metadata": {
        "id": "OsBunGDf7EBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "iQ9xRix-9vvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def package_tokenizer(prefix: str) -> Dict[str, Any]:\n",
        "    model_path = f\"{prefix}.model\"\n",
        "    vocab_path = f\"{prefix}.vocab\"\n",
        "    if not (os.path.isfile(model_path) and os.path.isfile(vocab_path)):\n",
        "        raise FileNotFoundError(f\"Missing tokenizer files at {prefix}.[model|vocab]\")\n",
        "    with open(model_path, \"rb\") as f:\n",
        "        model_bytes = f.read()\n",
        "    with open(vocab_path, \"rb\") as f:\n",
        "        vocab_bytes = f.read()\n",
        "    return {\"prefix\": prefix, \"model_bytes\": model_bytes, \"vocab_bytes\": vocab_bytes}"
      ],
      "metadata": {
        "id": "j-nb7fSf8ZrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = sp.get_piece_size()\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "model = TransformerModel(config, vocab_size).to(config.device)\n",
        "criterion = LabelSmoothedCrossEntropyLoss(config.label_smoothing, ignore_index = 0)\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = config.lr_base,\n",
        "                  betas = (0.9, 0.98),\n",
        "                  eps = 1e-9,\n",
        "                  weight_decay = config.weight_decay\n",
        ")\n",
        "scheduler = WarmupInverseSqrtScheduler(optimizer, config.warmup_steps, config.lr_base)\n",
        "tokenizer = package_tokenizer(config.spm_prefix)\n",
        "tokenizer_payload = package_tokenizer(config.spm_prefix)\n",
        "\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "print(f\"checkpoints will be saved to {config.save_dir}\")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_val_bleu = 0.0\n",
        "\n",
        "for epoch in range(1, config.num_epochs + 1):\n",
        "    train_loader, num_vi2zh_samples = build_train_loader(\n",
        "        train_dataset,\n",
        "        epoch=epoch,\n",
        "        config=config\n",
        "    )\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, config, epoch)\n",
        "    val_loss, val_bleu = evaluate(model, val_loader, criterion, sp, config)\n",
        "    print(f\"\\nEpoch {epoch}/{config.num_epochs}\")\n",
        "    print(f\"  vi→zh coverage: {num_vi2zh_samples}/{len(train_dataset.vi2zh_indices)} (~{100 * config.vi2zh_epoch_ratio:.1f}% per epoch)\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Valid Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Valid BLEU: {val_bleu:.2f}\")\n",
        "    print(f\"  Learning Rate: {scheduler.get_lr():.6f}\")\n",
        "\n",
        "    if epoch % config.save_every == 0:\n",
        "        ckpt_path = os.path.join(config.save_dir, f\"checkpoint_epoch_{epoch}.pt\")\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"train_loss\": train_loss,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_bleu\": val_bleu,\n",
        "                \"config\": config,\n",
        "                \"tokenizer\": tokenizer_payload,\n",
        "            },\n",
        "            ckpt_path,\n",
        "        )\n",
        "        print(f\"  ✓ Saved checkpoint to {ckpt_path}\")\n",
        "\n",
        "    # Update best model based on validation loss (always available)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_val_bleu = val_bleu\n",
        "\n",
        "        best_path = os.path.join(config.save_dir, \"best_model.pt\")\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"train_loss\": train_loss,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_bleu\": best_val_bleu,\n",
        "                \"config\": config,\n",
        "                \"tokenizer\": tokenizer_payload,\n",
        "            },\n",
        "            best_path,\n",
        "        )\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "print(f\"Best valid loss: {best_val_loss:.4f} | Best BLEU: {best_val_bleu:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "l4XnDC6B7J3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f407c1-64d4-4f96-869d-06bc97c722da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 8000\n",
            "Total parameters: 157179200\n",
            "checkpoints will be saved to ./checkpoint_bidirectional\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 171/171 [01:42<00:00,  1.66it/s, loss=4.6988, lr=0.000171]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 5.9643\n",
            "  Valid Loss: 0.6850\n",
            "  Valid BLEU: 4.48\n",
            "  Learning Rate: 0.000171\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 171/171 [01:45<00:00,  1.63it/s, loss=3.9446, lr=0.000153]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 4.1668\n",
            "  Valid Loss: 0.5514\n",
            "  Valid BLEU: 8.64\n",
            "  Learning Rate: 0.000153\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=3.2295, lr=0.000125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 3.4081\n",
            "  Valid Loss: 0.4874\n",
            "  Valid BLEU: 12.76\n",
            "  Learning Rate: 0.000125\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_3.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 171/171 [01:45<00:00,  1.61it/s, loss=2.9068, lr=0.000108]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 2.8454\n",
            "  Valid Loss: 0.4592\n",
            "  Valid BLEU: 14.99\n",
            "  Learning Rate: 0.000108\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_4.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=2.4609, lr=0.000097]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 2.3873\n",
            "  Valid Loss: 0.4449\n",
            "  Valid BLEU: 17.83\n",
            "  Learning Rate: 0.000097\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=2.1118, lr=0.000088]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 1.9992\n",
            "  Valid Loss: 0.4452\n",
            "  Valid BLEU: 18.53\n",
            "  Learning Rate: 0.000088\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_6.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 171/171 [01:44<00:00,  1.63it/s, loss=1.4749, lr=0.000082]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 1.6802\n",
            "  Valid Loss: 0.4463\n",
            "  Valid BLEU: 20.60\n",
            "  Learning Rate: 0.000082\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_7.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 171/171 [01:44<00:00,  1.63it/s, loss=1.5793, lr=0.000076]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 1.4142\n",
            "  Valid Loss: 0.4522\n",
            "  Valid BLEU: 20.92\n",
            "  Learning Rate: 0.000076\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_8.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 171/171 [01:45<00:00,  1.63it/s, loss=0.9938, lr=0.000072]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 1.1820\n",
            "  Valid Loss: 0.4535\n",
            "  Valid BLEU: 21.39\n",
            "  Learning Rate: 0.000072\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_9.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 171/171 [01:44<00:00,  1.63it/s, loss=0.8640, lr=0.000068]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.9987\n",
            "  Valid Loss: 0.4601\n",
            "  Valid BLEU: 21.94\n",
            "  Learning Rate: 0.000068\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_10.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=1.0307, lr=0.000065]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.8492\n",
            "  Valid Loss: 0.4628\n",
            "  Valid BLEU: 23.07\n",
            "  Learning Rate: 0.000065\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_11.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 171/171 [01:44<00:00,  1.63it/s, loss=0.7481, lr=0.000062]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.7123\n",
            "  Valid Loss: 0.4668\n",
            "  Valid BLEU: 23.78\n",
            "  Learning Rate: 0.000062\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_12.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.6852, lr=0.000060]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.6118\n",
            "  Valid Loss: 0.4703\n",
            "  Valid BLEU: 23.12\n",
            "  Learning Rate: 0.000060\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_13.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.4175, lr=0.000058]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.5241\n",
            "  Valid Loss: 0.4716\n",
            "  Valid BLEU: 23.30\n",
            "  Learning Rate: 0.000058\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_14.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 171/171 [01:45<00:00,  1.63it/s, loss=0.5021, lr=0.000056]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.4548\n",
            "  Valid Loss: 0.4761\n",
            "  Valid BLEU: 23.73\n",
            "  Learning Rate: 0.000056\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_15.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 171/171 [01:45<00:00,  1.63it/s, loss=0.3962, lr=0.000054]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.3941\n",
            "  Valid Loss: 0.4798\n",
            "  Valid BLEU: 22.31\n",
            "  Learning Rate: 0.000054\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_16.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 171/171 [01:45<00:00,  1.63it/s, loss=0.3708, lr=0.000052]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.3486\n",
            "  Valid Loss: 0.4847\n",
            "  Valid BLEU: 22.90\n",
            "  Learning Rate: 0.000052\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_17.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 171/171 [01:45<00:00,  1.63it/s, loss=0.3341, lr=0.000051]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.3097\n",
            "  Valid Loss: 0.4850\n",
            "  Valid BLEU: 23.12\n",
            "  Learning Rate: 0.000051\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_18.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 171/171 [01:44<00:00,  1.64it/s, loss=0.2346, lr=0.000050]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.2750\n",
            "  Valid Loss: 0.4899\n",
            "  Valid BLEU: 22.95\n",
            "  Learning Rate: 0.000050\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_19.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.2399, lr=0.000048]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.2506\n",
            "  Valid Loss: 0.4917\n",
            "  Valid BLEU: 23.09\n",
            "  Learning Rate: 0.000048\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_20.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.2199, lr=0.000047]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.2306\n",
            "  Valid Loss: 0.4913\n",
            "  Valid BLEU: 23.07\n",
            "  Learning Rate: 0.000047\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_21.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|██████████| 171/171 [01:44<00:00,  1.63it/s, loss=0.2622, lr=0.000046]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.2160\n",
            "  Valid Loss: 0.4933\n",
            "  Valid BLEU: 22.67\n",
            "  Learning Rate: 0.000046\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_22.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|██████████| 171/171 [01:44<00:00,  1.64it/s, loss=0.2052, lr=0.000045]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.2037\n",
            "  Valid Loss: 0.4954\n",
            "  Valid BLEU: 23.04\n",
            "  Learning Rate: 0.000045\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_23.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.1835, lr=0.000044]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1963\n",
            "  Valid Loss: 0.4952\n",
            "  Valid BLEU: 22.88\n",
            "  Learning Rate: 0.000044\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_24.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.1968, lr=0.000043]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1896\n",
            "  Valid Loss: 0.5009\n",
            "  Valid BLEU: 22.86\n",
            "  Learning Rate: 0.000043\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_25.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|██████████| 171/171 [01:46<00:00,  1.61it/s, loss=0.1942, lr=0.000042]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1863\n",
            "  Valid Loss: 0.4955\n",
            "  Valid BLEU: 22.88\n",
            "  Learning Rate: 0.000042\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_26.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|██████████| 171/171 [01:45<00:00,  1.63it/s, loss=0.1733, lr=0.000042]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1815\n",
            "  Valid Loss: 0.4973\n",
            "  Valid BLEU: 22.92\n",
            "  Learning Rate: 0.000042\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_27.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|██████████| 171/171 [01:45<00:00,  1.63it/s, loss=0.1724, lr=0.000041]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1792\n",
            "  Valid Loss: 0.4990\n",
            "  Valid BLEU: 23.59\n",
            "  Learning Rate: 0.000041\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_28.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.1788, lr=0.000040]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1770\n",
            "  Valid Loss: 0.5005\n",
            "  Valid BLEU: 23.53\n",
            "  Learning Rate: 0.000040\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_29.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|██████████| 171/171 [01:45<00:00,  1.63it/s, loss=0.1656, lr=0.000039]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1753\n",
            "  Valid Loss: 0.5013\n",
            "  Valid BLEU: 23.17\n",
            "  Learning Rate: 0.000039\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_30.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.1788, lr=0.000039]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 31/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1733\n",
            "  Valid Loss: 0.5017\n",
            "  Valid BLEU: 23.00\n",
            "  Learning Rate: 0.000039\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_31.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.1670, lr=0.000038]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 32/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1714\n",
            "  Valid Loss: 0.5020\n",
            "  Valid BLEU: 23.03\n",
            "  Learning Rate: 0.000038\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_32.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.1722, lr=0.000038]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 33/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1710\n",
            "  Valid Loss: 0.5066\n",
            "  Valid BLEU: 22.28\n",
            "  Learning Rate: 0.000038\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_33.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.1658, lr=0.000037]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 34/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1694\n",
            "  Valid Loss: 0.5016\n",
            "  Valid BLEU: 22.96\n",
            "  Learning Rate: 0.000037\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_34.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35: 100%|██████████| 171/171 [01:45<00:00,  1.63it/s, loss=0.1771, lr=0.000037]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 35/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1688\n",
            "  Valid Loss: 0.5043\n",
            "  Valid BLEU: 23.17\n",
            "  Learning Rate: 0.000037\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_35.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36: 100%|██████████| 171/171 [01:46<00:00,  1.61it/s, loss=0.1669, lr=0.000036]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 36/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1683\n",
            "  Valid Loss: 0.5037\n",
            "  Valid BLEU: 22.92\n",
            "  Learning Rate: 0.000036\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_36.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37: 100%|██████████| 171/171 [01:45<00:00,  1.63it/s, loss=0.1638, lr=0.000036]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 37/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1676\n",
            "  Valid Loss: 0.5099\n",
            "  Valid BLEU: 22.99\n",
            "  Learning Rate: 0.000036\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_37.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.1620, lr=0.000035]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 38/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1668\n",
            "  Valid Loss: 0.5089\n",
            "  Valid BLEU: 23.92\n",
            "  Learning Rate: 0.000035\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_38.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39: 100%|██████████| 171/171 [01:44<00:00,  1.63it/s, loss=0.1650, lr=0.000035]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 39/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1658\n",
            "  Valid Loss: 0.5074\n",
            "  Valid BLEU: 23.23\n",
            "  Learning Rate: 0.000035\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_39.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40: 100%|██████████| 171/171 [01:45<00:00,  1.62it/s, loss=0.1704, lr=0.000034]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 40/40\n",
            "  vi→zh coverage: 8977/12824 (~70.0% per epoch)\n",
            "  Train Loss: 0.1657\n",
            "  Valid Loss: 0.5055\n",
            "  Valid BLEU: 23.53\n",
            "  Learning Rate: 0.000034\n",
            "  ✓ Saved checkpoint to ./checkpoint_bidirectional/checkpoint_epoch_40.pt\n",
            "\n",
            "Training finished.\n",
            "Best valid loss: 0.4449 | Best BLEU: 17.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training stage 2\n",
        "- Using a small projector and training it to learn latent semantic space of both Chinese and Vietnamese\n",
        "- Objective: next token prediction as previous stage and teach model semantic feature of a sentence in both Vietnamese and Chinese\n",
        "- Train first 20 epochs apply Early Stopper(patience= 5)s to minimize loss function of Valid dataset(Stop at 10 epochs)\n",
        "- Train  next 20 epochs  apply Early Stopper(patience= 10)s to aximize sacrebleu score of Valid dataset(Stop at 12 epochs)\n",
        "- Totally, I trained my model for 22 epochs\n"
      ],
      "metadata": {
        "id": "_XpY12j6JDFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=5, min_delta=0, mode='min', save_path=None):\n",
        "        \"\"\"\n",
        "        mode: 'min' (giảm loss) hoặc 'max' (tăng BLEU)\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.save_path = save_path\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "        # Khởi tạo giá trị tốt nhất tùy theo mode\n",
        "        self.best_score = float('inf') if mode == 'min' else float('-inf')\n",
        "\n",
        "    def __call__(self, score, model, projection, optimizer, epoch, config, tokenizer):\n",
        "        if self.mode == 'min':\n",
        "            improved = score < self.best_score - self.min_delta\n",
        "        else:\n",
        "            improved = score > self.best_score + self.min_delta\n",
        "\n",
        "        if improved:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            print(f\" -> Score improved to {score:.4f}. Saving checkpoint...\")\n",
        "\n",
        "            # Lưu checkpoint đầy đủ\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'projection_state_dict': projection.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'score': score,\n",
        "                'config': config,\n",
        "                'tokenizer': tokenizer\n",
        "            }, self.save_path)\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\" -> EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ],
      "metadata": {
        "id": "DNRw8cdD5ISa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_evaluate(\n",
        "        model: TransformerModel,\n",
        "        dataloader: DataLoader,\n",
        "        criterion,\n",
        "        sp_model: spm.SentencePieceProcessor,\n",
        "        config: RopeConfig,\n",
        "        beam_size: int = 3,\n",
        "        max_bleu_samples:int = 400\n",
        "):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_references = []\n",
        "\n",
        "    bleu_count = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc = 'Validationg (Beam Search)', leave = False)\n",
        "\n",
        "    for src_batch, tgt_batch in pbar:\n",
        "        src_batch = src_batch.to(config.device)\n",
        "        tgt_batch = tgt_batch.to(config.device)\n",
        "\n",
        "        logits = model(src_batch, tgt_batch)\n",
        "        targets = tgt_batch[:, 1:]\n",
        "        loss = criterion(logits.reshape(-1,logits.size(-1)), targets.reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if bleu_count < max_bleu_samples:\n",
        "            preds = beam_search_decode(\n",
        "                model = model,\n",
        "                src_ids=src_batch,\n",
        "                sp_model = sp_model,\n",
        "                config = config,\n",
        "                beam_size = beam_size,\n",
        "                top_k = 3,\n",
        "                max_len = config.max_len,\n",
        "                length_penalty = 0.6\n",
        "            )\n",
        "\n",
        "            eos = sp_model.piece_to_id(config.eos_token)\n",
        "            for tgt_seq in tgt_batch:\n",
        "                ref_ids = tgt_seq[1:].cpu().tolist()\n",
        "                if eos in ref_ids:\n",
        "                    ref_ids = ref_ids[:ref_ids.index(eos)]\n",
        "\n",
        "                ref_text = sp_model.decode(ref_ids)\n",
        "                all_references.append(ref_text)\n",
        "\n",
        "            all_predictions.extend(preds)\n",
        "            bleu_count += len(preds)\n",
        "\n",
        "            if bleu_count >= max_bleu_samples:\n",
        "                pbar.set_description('Validation (Beam Search)')\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    bleu_score = 0.0\n",
        "    if all_predictions:\n",
        "        try:\n",
        "            #Scarebleu require references having type of list of lists\n",
        "            bleu = sacrebleu.corpus_bleu(all_predictions, [all_references], force = True)\n",
        "            bleu_score = bleu.score\n",
        "\n",
        "            # In thử 1 câu mẫu để debug\n",
        "            print(f\"\\nExample Pred: {all_predictions[0]}\")\n",
        "            print(f\"Example Ref : {all_references[0]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Lỗi tính BLEU: {e}\")\n",
        "\n",
        "    return avg_loss, bleu_score\n"
      ],
      "metadata": {
        "id": "b7EGK5nk6jN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "GeS4vEdXVhAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CONTRASTIVE LEARNING SETUP =====\n",
        "\n",
        "# Contrastive training config (define first to avoid memory issues)\n",
        "@dataclass\n",
        "class ContrastiveConfig:\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    proj_dim: int = 768\n",
        "    contrastive_tau: float = 0.07\n",
        "    cross_lambda_max: float = 0.1\n",
        "    cross_warmup_steps: int = 200\n",
        "    lr_base: float = 5e-5\n",
        "    warmup_steps: int = 200\n",
        "    num_epochs: int = 20\n",
        "    batch_size: int = 64\n",
        "    save_dir: str = \"./checkpoints_contrastive\"\n",
        "    save_every: int = 5\n",
        "\n",
        "cl_config = ContrastiveConfig()\n",
        "os.makedirs(cl_config.save_dir, exist_ok=True)\n",
        "\n",
        "# Load checkpoint epoch 40 for contrastive training (load to CPU first)\n",
        "print(\"Loading checkpoint for contrastive training...\")\n",
        "checkpoint_path = os.path.join(config.save_dir, \"checkpoint_epoch_40.pt\")\n",
        "\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    raise FileNotFoundError(f\"Required checkpoint not found: {checkpoint_path}\")\n",
        "\n",
        "print(f\"Loading checkpoint: {checkpoint_path}\")\n",
        "# Load to CPU first to avoid GPU memory issues\n",
        "checkpoint = torch.load(checkpoint_path, map_location=config.device, weights_only=False)\n",
        "\n",
        "# Update config with smaller batch size for contrastive learning\n",
        "old_config = checkpoint[\"config\"]\n",
        "config.batch_size = cl_config.batch_size  # Use smaller batch size\n",
        "config.num_workers = 2  # Reduce workers\n",
        "\n",
        "# Load model state dict\n",
        "# model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "# print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
        "# print(f\"Updated batch_size from {old_config.batch_size} to {config.batch_size}\")\n",
        "\n",
        "# print(f\"Contrastive config: lr={cl_config.lr_base}, epochs={cl_config.num_epochs}\")\n",
        "# print(f\"Save dir: {cl_config.save_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT5n3mcD2jA4",
        "outputId": "be9d4ec4-579e-4cc9-a2b4-94725a63c168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint for contrastive training...\n",
            "Loading checkpoint: ./checkpoint_bidirectional/checkpoint_epoch_40.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "projection = ProjectionHead(config.d_model, cl_config.proj_dim).to(config.device)\n",
        "print(f\"Projection head: {config.d_model} -> {cl_config.proj_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFKzA41A1cd0",
        "outputId": "96b068fa-19e8-4235-dea6-c8f12d5d0cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projection head: 768 -> 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def package_tokenizer(prefix: str) -> Dict[str, Any]:\n",
        "    model_path = f\"{prefix}.model\"\n",
        "    vocab_path = f\"{prefix}.vocab\"\n",
        "    if not (os.path.isfile(model_path) and os.path.isfile(vocab_path)):\n",
        "        raise FileNotFoundError(f\"Missing tokenizer files at {prefix}.[model|vocab]\")\n",
        "    with open(model_path, \"rb\") as f:\n",
        "        model_bytes = f.read()\n",
        "    with open(vocab_path, \"rb\") as f:\n",
        "        vocab_bytes = f.read()\n",
        "    return {\"prefix\": prefix, \"model_bytes\": model_bytes, \"vocab_bytes\": vocab_bytes}"
      ],
      "metadata": {
        "id": "zg8nLFlcSKag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Khởi tạo Model & Optimizer ---\n",
        "vocab_size = sp.get_piece_size()\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "model = TransformerModel(config, vocab_size ).to(config.device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "projection = ProjectionHead(config.d_model, cl_config.proj_dim).to(config.device)\n",
        "\n",
        "\n",
        "optimizer = AdamW(\n",
        "    list(model.parameters()) + list(projection.parameters()),\n",
        "    lr=cl_config.lr_base,\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "tokenizer_payload = package_tokenizer(config.spm_prefix)\n",
        "\n",
        "\n",
        "# warmup_steps thường là 4000 hoặc 10% tổng số bước train\n",
        "scheduler = WarmupInverseSqrtScheduler(\n",
        "    optimizer=optimizer,\n",
        "    warmup_steps=4000,\n",
        "    lr_base=0.0005\n",
        ")\n",
        "\n",
        "# --- 3. Cấu hình Early Stopping ---\n",
        "early_stopper = EarlyStopper(\n",
        "    patience=5,\n",
        "    min_delta=0.0001,\n",
        "    mode='min',\n",
        "    save_path=os.path.join(cl_config.save_dir, \"best_cl_model.pt\")\n",
        ")\n",
        "\n",
        "criterion = LabelSmoothedCrossEntropyLoss(config.label_smoothing, ignore_index = 0)\n",
        "\n",
        "special_ids = [\n",
        "    sp.piece_to_id(config.pad_token),\n",
        "    sp.piece_to_id(config.bos_token),\n",
        "    sp.piece_to_id(config.eos_token),\n",
        "    sp.piece_to_id(config.zh_token),\n",
        "    sp.piece_to_id(config.vi_token)\n",
        "]\n",
        "print(f\"Special token IDs: {special_ids}\")\n",
        "\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())+ sum(j.numel() for j in projection.parameters())}\")\n",
        "print(\"Start_Training\")\n",
        "\n",
        "global_step = 0 # Initialize global_step here\n",
        "\n",
        "# Re-initialize val_loader and test_loader to ensure correct collate_fn\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn_eval\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn_eval\n",
        ")\n",
        "\n",
        "for epoch in range(1, cl_config.num_epochs + 1):\n",
        "    train_loader, num_vi2zh_samples = build_train_loader(\n",
        "        train_dataset,\n",
        "        epoch=epoch,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*10} EPOCH {epoch} {'='*10}\")\n",
        "    # Hàm train_epoch của bạn đã có dòng `scheduler.step()` bên trong vòng lặp batch\n",
        "    avg_ce_loss, avg_cl_loss, avg_total_loss, global_step = contrastive_train_epoch(\n",
        "        model = model,\n",
        "        projection = projection,\n",
        "        dataloader = train_loader,\n",
        "        optimizer = optimizer,\n",
        "        criterion = criterion,\n",
        "        scheduler = scheduler,\n",
        "        cl_config = cl_config,\n",
        "        pad_id = sp.piece_to_id(config.pad_token),\n",
        "        special_ids = special_ids,\n",
        "        global_step = global_step,\n",
        "        epoch = epoch\n",
        "    )\n",
        "\n",
        "    print(f\"\\nEpoch {epoch}/{cl_config.num_epochs}\")\n",
        "    print(f\" - Train CE Loss: {avg_ce_loss:.4f}\")\n",
        "    print(f\" - Train CL Loss: {avg_cl_loss:.4f}\")\n",
        "    print(f\" - Total Loss:    {avg_total_loss:.4f}\")\n",
        "    print(f\" - Learning Rate: {scheduler.get_lr():.8f}\")\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_bleu = evaluate(model, val_loader, criterion, sp, config)\n",
        "\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val BLEU: {val_bleu:.2f}\")\n",
        "\n",
        "    if epoch % cl_config.save_every == 0:\n",
        "        save_path = os.path.join(cl_config.save_dir, f\"cl_checkpoint_epoch_{epoch}.pt\")\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"projection_state_dict\": projection.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"loss\": avg_total_loss,\n",
        "            \"config\": config,\n",
        "            \"tokenizer\": tokenizer_payload\n",
        "        }, save_path)\n",
        "        print(f\" -> Saved periodic checkpoint: {save_path}\")\n",
        "\n",
        "    # Check Early Stopping\n",
        "    early_stopper(\n",
        "        score = val_loss,\n",
        "        model = model,\n",
        "        projection = projection,\n",
        "        optimizer = optimizer,\n",
        "        epoch = epoch,\n",
        "        config = config,\n",
        "        tokenizer = tokenizer_payload\n",
        "    )\n",
        "\n",
        "    if early_stopper.early_stop:\n",
        "        print(f\"Early stopping triggered at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "print(\"Contrastive Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q37TW2g4C_Z7",
        "outputId": "6b231acd-9954-4eda-e6c5-55beeff513cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 8000\n",
            "Special token IDs: [0, 2, 3, 4, 5]\n",
            "Total parameters: 158360384\n",
            "Start_Training\n",
            "\n",
            "========== EPOCH 1 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 341/341 [02:55<00:00,  1.95it/s, ce=0.2048, cl=0.0996, λ=0.100, lr=0.000043]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            " - Train CE Loss: 0.1651\n",
            " - Train CL Loss: 1.1635\n",
            " - Total Loss:    0.2069\n",
            " - Learning Rate: 0.00004263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5177 | Val BLEU: 23.59\n",
            " -> Score improved to 0.5177. Saving checkpoint...\n",
            "\n",
            "========== EPOCH 2 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 341/341 [02:57<00:00,  1.92it/s, ce=0.2300, cl=0.0375, λ=0.100, lr=0.000085]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/20\n",
            " - Train CE Loss: 0.1894\n",
            " - Train CL Loss: 0.1019\n",
            " - Total Loss:    0.1996\n",
            " - Learning Rate: 0.00008525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5216 | Val BLEU: 21.29\n",
            " -> EarlyStopping counter: 1 out of 5\n",
            "\n",
            "========== EPOCH 3 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 341/341 [02:57<00:00,  1.92it/s, ce=0.2686, cl=0.0504, λ=0.100, lr=0.000128]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/20\n",
            " - Train CE Loss: 0.2551\n",
            " - Train CL Loss: 0.0689\n",
            " - Total Loss:    0.2619\n",
            " - Learning Rate: 0.00012788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5229 | Val BLEU: 20.97\n",
            " -> EarlyStopping counter: 2 out of 5\n",
            "\n",
            "========== EPOCH 4 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 341/341 [02:56<00:00,  1.93it/s, ce=0.3540, cl=0.0588, λ=0.100, lr=0.000171]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/20\n",
            " - Train CE Loss: 0.3653\n",
            " - Train CL Loss: 0.0671\n",
            " - Total Loss:    0.3720\n",
            " - Learning Rate: 0.00017050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5080 | Val BLEU: 19.72\n",
            " -> Score improved to 0.5080. Saving checkpoint...\n",
            "\n",
            "========== EPOCH 5 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 341/341 [02:57<00:00,  1.92it/s, ce=0.5803, cl=0.0386, λ=0.100, lr=0.000213]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/20\n",
            " - Train CE Loss: 0.4886\n",
            " - Train CL Loss: 0.0725\n",
            " - Total Loss:    0.4959\n",
            " - Learning Rate: 0.00021313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5044 | Val BLEU: 18.77\n",
            " -> Saved periodic checkpoint: ./checkpoints_contrastive/cl_checkpoint_epoch_5.pt\n",
            " -> Score improved to 0.5044. Saving checkpoint...\n",
            "\n",
            "========== EPOCH 6 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 341/341 [02:58<00:00,  1.91it/s, ce=0.6358, cl=0.1342, λ=0.100, lr=0.000256]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/20\n",
            " - Train CE Loss: 0.5979\n",
            " - Train CL Loss: 0.0802\n",
            " - Total Loss:    0.6059\n",
            " - Learning Rate: 0.00025575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4899 | Val BLEU: 18.42\n",
            " -> Score improved to 0.4899. Saving checkpoint...\n",
            "\n",
            "========== EPOCH 7 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 341/341 [02:58<00:00,  1.91it/s, ce=0.8394, cl=0.1064, λ=0.100, lr=0.000298]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/20\n",
            " - Train CE Loss: 0.6680\n",
            " - Train CL Loss: 0.0854\n",
            " - Total Loss:    0.6766\n",
            " - Learning Rate: 0.00029837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4810 | Val BLEU: 20.02\n",
            " -> Score improved to 0.4810. Saving checkpoint...\n",
            "\n",
            "========== EPOCH 8 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 341/341 [02:59<00:00,  1.90it/s, ce=0.8467, cl=0.0392, λ=0.100, lr=0.000341]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/20\n",
            " - Train CE Loss: 0.7228\n",
            " - Train CL Loss: 0.0906\n",
            " - Total Loss:    0.7319\n",
            " - Learning Rate: 0.00034100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4862 | Val BLEU: 18.46\n",
            " -> EarlyStopping counter: 1 out of 5\n",
            "\n",
            "========== EPOCH 9 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 341/341 [02:59<00:00,  1.90it/s, ce=0.9475, cl=0.0350, λ=0.100, lr=0.000384]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/20\n",
            " - Train CE Loss: 0.7599\n",
            " - Train CL Loss: 0.0912\n",
            " - Total Loss:    0.7690\n",
            " - Learning Rate: 0.00038363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4895 | Val BLEU: 16.83\n",
            " -> EarlyStopping counter: 2 out of 5\n",
            "\n",
            "========== EPOCH 10 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 341/341 [02:58<00:00,  1.91it/s, ce=1.0130, cl=0.0658, λ=0.100, lr=0.000426]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/20\n",
            " - Train CE Loss: 0.7924\n",
            " - Train CL Loss: 0.0948\n",
            " - Total Loss:    0.8019\n",
            " - Learning Rate: 0.00042625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5009 | Val BLEU: 18.48\n",
            " -> Saved periodic checkpoint: ./checkpoints_contrastive/cl_checkpoint_epoch_10.pt\n",
            " -> EarlyStopping counter: 3 out of 5\n",
            "\n",
            "========== EPOCH 11 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 341/341 [02:58<00:00,  1.91it/s, ce=0.9772, cl=0.0379, λ=0.100, lr=0.000469]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/20\n",
            " - Train CE Loss: 0.8121\n",
            " - Train CL Loss: 0.0949\n",
            " - Total Loss:    0.8216\n",
            " - Learning Rate: 0.00046887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5007 | Val BLEU: 17.61\n",
            " -> EarlyStopping counter: 4 out of 5\n",
            "\n",
            "========== EPOCH 12 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 341/341 [02:58<00:00,  1.91it/s, ce=0.9787, cl=0.1168, λ=0.100, lr=0.000494]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/20\n",
            " - Train CE Loss: 0.8276\n",
            " - Train CL Loss: 0.0946\n",
            " - Total Loss:    0.8371\n",
            " - Learning Rate: 0.00049435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5000 | Val BLEU: 18.42\n",
            " -> EarlyStopping counter: 5 out of 5\n",
            "Early stopping triggered at epoch 12\n",
            "Contrastive Training finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 2.1: Continue to train for 20 epochs applied early stopper to maximize sacrebleu. Stop at epoch 12"
      ],
      "metadata": {
        "id": "qWx5a3mVeCjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ContrastiveConfig:\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    proj_dim: int = 768\n",
        "    contrastive_tau: float = 0.07\n",
        "    cross_lambda_max: float = 0.1\n",
        "    cross_warmup_steps: int = 200\n",
        "    lr_base: float = 5e-5\n",
        "    warmup_steps: int = 200\n",
        "    num_epochs: int = 20\n",
        "    batch_size: int = 64\n",
        "    save_dir: str = \"./checkpoints_contrastive\"\n",
        "    save_every: int = 5\n",
        "\n",
        "cl_config = ContrastiveConfig()\n",
        "os.makedirs(cl_config.save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "QREem3safj9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Khởi tạo Model & Optimizer ---\n",
        "vocab_size = sp.get_piece_size()\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "model = TransformerModel(config, vocab_size ).to(config.device)\n",
        "checkpoint_path = '/content/checkpoints_contrastive/cl_checkpoint_epoch_10.pt'\n",
        "checkpoint = torch.load(checkpoint_path, map_location=config.device, weights_only=False)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "projection = ProjectionHead(config.d_model, cl_config.proj_dim).to(config.device)\n",
        "projection.load_state_dict(checkpoint[\"projection_state_dict\"])\n",
        "\n",
        "\n",
        "optimizer = AdamW(\n",
        "    list(model.parameters()) + list(projection.parameters()),\n",
        "    lr=cl_config.lr_base,\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "tokenizer_payload = package_tokenizer(config.spm_prefix)\n",
        "\n",
        "\n",
        "# warmup_steps thường là 4000 hoặc 10% tổng số bước train\n",
        "scheduler = WarmupInverseSqrtScheduler(\n",
        "    optimizer=optimizer,\n",
        "    warmup_steps=4000,\n",
        "    lr_base=0.0005\n",
        ")\n",
        "\n",
        "# --- 3. Cấu hình Early Stopping ---\n",
        "early_stopper = EarlyStopper(\n",
        "    patience=10,\n",
        "    min_delta=0.0001,\n",
        "    mode='max',\n",
        "    save_path=os.path.join(cl_config.save_dir, \"best_cl_model.pt\")\n",
        ")\n",
        "\n",
        "criterion = LabelSmoothedCrossEntropyLoss(config.label_smoothing, ignore_index = 0)\n",
        "\n",
        "special_ids = [\n",
        "    sp.piece_to_id(config.pad_token),\n",
        "    sp.piece_to_id(config.bos_token),\n",
        "    sp.piece_to_id(config.eos_token),\n",
        "    sp.piece_to_id(config.zh_token),\n",
        "    sp.piece_to_id(config.vi_token)\n",
        "]\n",
        "print(f\"Special token IDs: {special_ids}\")\n",
        "\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())+ sum(j.numel() for j in projection.parameters())}\")\n",
        "print(\"Start_Training\")\n",
        "\n",
        "global_step = 0 # Initialize global_step here\n",
        "\n",
        "# Re-initialize val_loader and test_loader to ensure correct collate_fn\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn_eval\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn_eval\n",
        ")\n",
        "\n",
        "for epoch in range(1, cl_config.num_epochs + 1):\n",
        "    train_loader, num_vi2zh_samples = build_train_loader(\n",
        "        train_dataset,\n",
        "        epoch=epoch,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*10} EPOCH {epoch + 10} {'='*10}\")\n",
        "    # Hàm train_epoch của bạn đã có dòng `scheduler.step()` bên trong vòng lặp batch\n",
        "    avg_ce_loss, avg_cl_loss, avg_total_loss, global_step = contrastive_train_epoch(\n",
        "        model = model,\n",
        "        projection = projection,\n",
        "        dataloader = train_loader,\n",
        "        optimizer = optimizer,\n",
        "        criterion = criterion,\n",
        "        scheduler = scheduler,\n",
        "        cl_config = cl_config,\n",
        "        pad_id = sp.piece_to_id(config.pad_token),\n",
        "        special_ids = special_ids,\n",
        "        global_step = global_step,\n",
        "        epoch = epoch\n",
        "    )\n",
        "\n",
        "    print(f\"\\nEpoch {epoch}/{cl_config.num_epochs}\")\n",
        "    print(f\" - Train CE Loss: {avg_ce_loss:.4f}\")\n",
        "    print(f\" - Train CL Loss: {avg_cl_loss:.4f}\")\n",
        "    print(f\" - Total Loss:    {avg_total_loss:.4f}\")\n",
        "    print(f\" - Learning Rate: {scheduler.get_lr():.8f}\")\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_bleu = evaluate(model, val_loader, criterion, sp, config)\n",
        "\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val BLEU: {val_bleu:.2f}\")\n",
        "\n",
        "    if epoch % cl_config.save_every == 0:\n",
        "        save_path = os.path.join(cl_config.save_dir, f\"cl_checkpoint_epoch_{epoch}.pt\")\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"projection_state_dict\": projection.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"loss\": avg_total_loss,\n",
        "            \"config\": config,\n",
        "            \"tokenizer\": tokenizer_payload\n",
        "        }, save_path)\n",
        "        print(f\" -> Saved periodic checkpoint: {save_path}\")\n",
        "\n",
        "    # Check Early Stopping\n",
        "    early_stopper(\n",
        "        score = val_bleu,\n",
        "        model = model,\n",
        "        projection = projection,\n",
        "        optimizer = optimizer,\n",
        "        epoch = epoch,\n",
        "        config = config,\n",
        "        tokenizer = tokenizer_payload\n",
        "    )\n",
        "\n",
        "    if early_stopper.early_stop:\n",
        "        print(f\"Early stopping triggered at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "print(\"Contrastive Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpfjEK9Leomb",
        "outputId": "f01b12b1-8405-4dd8-dbd8-60baa44af694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 8000\n",
            "Special token IDs: [0, 2, 3, 4, 5]\n",
            "Total parameters: 158360384\n",
            "Start_Training\n",
            "\n",
            "========== EPOCH 11 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 341/341 [02:55<00:00,  1.94it/s, ce=0.4030, cl=0.0123, λ=0.100, lr=0.000043]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            " - Train CE Loss: 0.4800\n",
            " - Train CL Loss: 0.0617\n",
            " - Total Loss:    0.4839\n",
            " - Learning Rate: 0.00004263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4681 | Val BLEU: 22.78\n",
            " -> Score improved to 22.7814. Saving checkpoint...\n",
            "\n",
            "========== EPOCH 12 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 341/341 [02:58<00:00,  1.91it/s, ce=0.2686, cl=0.0279, λ=0.100, lr=0.000085]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/20\n",
            " - Train CE Loss: 0.2644\n",
            " - Train CL Loss: 0.0321\n",
            " - Total Loss:    0.2676\n",
            " - Learning Rate: 0.00008525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4760 | Val BLEU: 23.46\n",
            " -> Score improved to 23.4603. Saving checkpoint...\n",
            "\n",
            "========== EPOCH 13 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 341/341 [02:59<00:00,  1.90it/s, ce=0.2520, cl=0.0172, λ=0.100, lr=0.000128]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/20\n",
            " - Train CE Loss: 0.2243\n",
            " - Train CL Loss: 0.0250\n",
            " - Total Loss:    0.2268\n",
            " - Learning Rate: 0.00012788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4935 | Val BLEU: 22.75\n",
            " -> EarlyStopping counter: 1 out of 10\n",
            "\n",
            "========== EPOCH 14 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 341/341 [02:59<00:00,  1.90it/s, ce=0.2317, cl=0.0347, λ=0.100, lr=0.000171]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/20\n",
            " - Train CE Loss: 0.2431\n",
            " - Train CL Loss: 0.0237\n",
            " - Total Loss:    0.2455\n",
            " - Learning Rate: 0.00017050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4959 | Val BLEU: 23.14\n",
            " -> EarlyStopping counter: 2 out of 10\n",
            "\n",
            "========== EPOCH 15 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 341/341 [02:58<00:00,  1.91it/s, ce=0.3603, cl=0.0226, λ=0.100, lr=0.000213]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/20\n",
            " - Train CE Loss: 0.2651\n",
            " - Train CL Loss: 0.0250\n",
            " - Total Loss:    0.2676\n",
            " - Learning Rate: 0.00021313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5087 | Val BLEU: 22.27\n",
            " -> Saved periodic checkpoint: ./checkpoints_contrastive/cl_checkpoint_epoch_5.pt\n",
            " -> EarlyStopping counter: 3 out of 10\n",
            "\n",
            "========== EPOCH 16 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 341/341 [02:59<00:00,  1.90it/s, ce=0.2752, cl=0.0220, λ=0.100, lr=0.000256]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/20\n",
            " - Train CE Loss: 0.2948\n",
            " - Train CL Loss: 0.0285\n",
            " - Total Loss:    0.2976\n",
            " - Learning Rate: 0.00025575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5148 | Val BLEU: 21.47\n",
            " -> EarlyStopping counter: 4 out of 10\n",
            "\n",
            "========== EPOCH 17 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 341/341 [02:58<00:00,  1.91it/s, ce=0.3892, cl=0.0315, λ=0.100, lr=0.000298]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/20\n",
            " - Train CE Loss: 0.3398\n",
            " - Train CL Loss: 0.0336\n",
            " - Total Loss:    0.3432\n",
            " - Learning Rate: 0.00029837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5194 | Val BLEU: 20.43\n",
            " -> EarlyStopping counter: 5 out of 10\n",
            "\n",
            "========== EPOCH 18 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 341/341 [02:59<00:00,  1.90it/s, ce=0.5271, cl=0.0205, λ=0.100, lr=0.000341]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/20\n",
            " - Train CE Loss: 0.3934\n",
            " - Train CL Loss: 0.0393\n",
            " - Total Loss:    0.3974\n",
            " - Learning Rate: 0.00034100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5103 | Val BLEU: 20.08\n",
            " -> EarlyStopping counter: 6 out of 10\n",
            "\n",
            "========== EPOCH 19 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 341/341 [02:59<00:00,  1.90it/s, ce=0.5182, cl=0.0261, λ=0.100, lr=0.000384]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/20\n",
            " - Train CE Loss: 0.4427\n",
            " - Train CL Loss: 0.0466\n",
            " - Total Loss:    0.4473\n",
            " - Learning Rate: 0.00038363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5171 | Val BLEU: 19.33\n",
            " -> EarlyStopping counter: 7 out of 10\n",
            "\n",
            "========== EPOCH 20 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 341/341 [02:58<00:00,  1.91it/s, ce=0.5901, cl=0.0465, λ=0.100, lr=0.000426]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/20\n",
            " - Train CE Loss: 0.4987\n",
            " - Train CL Loss: 0.0541\n",
            " - Total Loss:    0.5041\n",
            " - Learning Rate: 0.00042625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5204 | Val BLEU: 20.19\n",
            " -> Saved periodic checkpoint: ./checkpoints_contrastive/cl_checkpoint_epoch_10.pt\n",
            " -> EarlyStopping counter: 8 out of 10\n",
            "\n",
            "========== EPOCH 21 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 341/341 [02:59<00:00,  1.90it/s, ce=0.5524, cl=0.0230, λ=0.100, lr=0.000469]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/20\n",
            " - Train CE Loss: 0.5594\n",
            " - Train CL Loss: 0.0593\n",
            " - Total Loss:    0.5653\n",
            " - Learning Rate: 0.00046887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5303 | Val BLEU: 19.70\n",
            " -> EarlyStopping counter: 9 out of 10\n",
            "\n",
            "========== EPOCH 22 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 341/341 [02:58<00:00,  1.91it/s, ce=0.7512, cl=0.0231, λ=0.100, lr=0.000494]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/20\n",
            " - Train CE Loss: 0.5977\n",
            " - Train CL Loss: 0.0653\n",
            " - Total Loss:    0.6043\n",
            " - Learning Rate: 0.00049435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5288 | Val BLEU: 20.02\n",
            " -> EarlyStopping counter: 10 out of 10\n",
            "Early stopping triggered at epoch 12\n",
            "Contrastive Training finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# 1. Xóa các biến không còn dùng (nếu bạn biết tên cụ thể)\n",
        "# Ví dụ: del model, optimizer, loss\n",
        "# del ten_bien_lon_cua_ban\n",
        "\n",
        "# 2. Thu gom rác của Python\n",
        "gc.collect()\n",
        "\n",
        "# 3. Xóa bộ nhớ cache của CUDA\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Kiểm tra lại dung lượng\n",
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4XIBtTG0_cv",
        "outputId": "65021ea7-7d2f-4eff-e396-36ed43605275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   4303 MiB |   9458 MiB |   5336 GiB |   5331 GiB |\n",
            "|       from large pool |   4048 MiB |   9159 MiB |   5106 GiB |   5102 GiB |\n",
            "|       from small pool |    254 MiB |    437 MiB |    230 GiB |    229 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   4303 MiB |   9458 MiB |   5336 GiB |   5331 GiB |\n",
            "|       from large pool |   4048 MiB |   9159 MiB |   5106 GiB |   5102 GiB |\n",
            "|       from small pool |    254 MiB |    437 MiB |    230 GiB |    229 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |   4275 MiB |   9331 MiB |   5186 GiB |   5181 GiB |\n",
            "|       from large pool |   4021 MiB |   9025 MiB |   4956 GiB |   4952 GiB |\n",
            "|       from small pool |    254 MiB |    437 MiB |    230 GiB |    229 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5368 MiB |  10028 MiB |  10028 MiB |   4660 MiB |\n",
            "|       from large pool |   5020 MiB |   9504 MiB |   9504 MiB |   4484 MiB |\n",
            "|       from small pool |    348 MiB |    524 MiB |    524 MiB |    176 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1064 MiB |   1635 MiB |   4462 GiB |   4461 GiB |\n",
            "|       from large pool |    971 MiB |   1543 MiB |   4213 GiB |   4212 GiB |\n",
            "|       from small pool |     93 MiB |    108 MiB |    249 GiB |    249 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1501    |    2470    |    1768 K  |    1766 K  |\n",
            "|       from large pool |     578    |    1340    |    1093 K  |    1093 K  |\n",
            "|       from small pool |     923    |    1352    |     674 K  |     673 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1501    |    2470    |    1768 K  |    1766 K  |\n",
            "|       from large pool |     578    |    1340    |    1093 K  |    1093 K  |\n",
            "|       from small pool |     923    |    1352    |     674 K  |     673 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     425    |     688    |     688    |     263    |\n",
            "|       from large pool |     251    |     426    |     426    |     175    |\n",
            "|       from small pool |     174    |     262    |     262    |      88    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |     408    |     478    |    1041 K  |    1041 K  |\n",
            "|       from large pool |     186    |     241    |     818 K  |     817 K  |\n",
            "|       from small pool |     222    |     284    |     223 K  |     223 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test best model after contrastive + next token prediction training with test set"
      ],
      "metadata": {
        "id": "X9VxX-JstV6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the best contrastive model\n",
        "best_model_path = os.path.join(cl_config.save_dir, \"best_cl_model.pt\")\n",
        "print(f\"Loading best model from: {best_model_path}\")\n",
        "\n",
        "if not os.path.exists(best_model_path):\n",
        "    raise FileNotFoundError(f\"Best contrastive model not found at {best_model_path}\")\n",
        "\n",
        "best_checkpoint = torch.load(best_model_path, map_location=config.device, weights_only=False)\n",
        "\n",
        "# Initialize model and projection head\n",
        "model = TransformerModel(config, vocab_size).to(config.device)\n",
        "projection = ProjectionHead(config.d_model, cl_config.proj_dim).to(config.device)\n",
        "\n",
        "# Load state dictionaries\n",
        "model.load_state_dict(best_checkpoint[\"model_state_dict\"])\n",
        "projection.load_state_dict(best_checkpoint[\"projection_state_dict\"])\n",
        "\n",
        "print(\"Evaluating best model on test set...\")\n",
        "test_loss, test_bleu = beam_evaluate(model, test_loader, criterion, sp, config, beam_size=3)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f} | Test BLEU: {test_bleu:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG1HxoNbtVl7",
        "outputId": "d5349afa-498b-4e01-d8b9-62674d016e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading best model from: ./checkpoints_contrastive/best_cl_model.pt\n",
            "Evaluating best model on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example Pred: Đó không phải hoàn_toàn là những gì tôi muốn .\n",
            "Example Ref : Nó không phải là thứ tôi muốn xem .\n",
            "\n",
            "Test Loss: 3.1104 | Test BLEU: 24.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}